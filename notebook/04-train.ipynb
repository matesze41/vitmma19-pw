{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dad39b4",
   "metadata": {},
   "source": [
    "# Train a small transformer model on preprocessed segments\n",
    "\n",
    "This notebook loads `segments_preproc_24.csv` produced by the preprocessing notebook, builds segment-level sequences, and trains a small transformer-based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7c6660e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using preprocessed file: /work/data/export/segments_preproc_24.csv\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports and configuration\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "BASE_DATA_DIR = os.path.abspath(\"../data\")\n",
    "EXPORT_DIR = os.path.join(BASE_DATA_DIR, \"export\")\n",
    "PREPROC_CSV = os.path.join(EXPORT_DIR, \"segments_preproc_24.csv\")\n",
    "\n",
    "print(\"Using preprocessed file:\", PREPROC_CSV)\n",
    "assert os.path.exists(PREPROC_CSV), f\"Preprocessed CSV not found: {PREPROC_CSV}\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7431e368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw preprocessed shape: (1392, 12)\n",
      "Feature columns ( 8 ): ['Open_norm', 'High_norm', 'Low_norm', 'Close_norm', 'vol_close', 'vol_high_low', 'compression_ratio', 'trend']\n",
      "Num segments: 58 Seq len: 24 Num features: 8\n",
      "Label distribution:\n",
      "Bearish Pennant    15\n",
      "Bullish Normal     14\n",
      "Bullish Wedge       9\n",
      "Bullish Pennant     9\n",
      "Bearish Wedge       6\n",
      "Bearish Normal      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed dataset and build segment-level sequences\n",
    "df = pd.read_csv(PREPROC_CSV)\n",
    "print(\"Raw preprocessed shape:\", df.shape)\n",
    "\n",
    "# Ensure correct ordering within each segment\n",
    "df = df.sort_values([\"segment_id\", \"seq_pos\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in [\"segment_id\", \"label\", \"csv_file\", \"seq_pos\"]]\n",
    "print(\"Feature columns (\", len(feature_cols), \"):\", feature_cols)\n",
    "\n",
    "# Group into (segment, sequence of length 24, label)\n",
    "segments = []\n",
    "labels = []\n",
    "\n",
    "for seg_id, g in df.groupby(\"segment_id\", sort=True):\n",
    "    g = g.sort_values(\"seq_pos\", kind=\"mergesort\")\n",
    "    feat = g[feature_cols].to_numpy(dtype=np.float32)\n",
    "    # Expect 24 steps; if shorter/longer, adjust with simple strategies\n",
    "    if feat.shape[0] < 24:\n",
    "        # pad by repeating last step\n",
    "        pad = np.repeat(feat[-1:, :], 24 - feat.shape[0], axis=0)\n",
    "        feat = np.concatenate([feat, pad], axis=0)\n",
    "    elif feat.shape[0] > 24:\n",
    "        # truncate extra steps\n",
    "        feat = feat[:24, :]\n",
    "\n",
    "    assert feat.shape[0] == 24, feat.shape\n",
    "    segments.append(feat)\n",
    "    labels.append(g[\"label\"].iloc[0])\n",
    "\n",
    "X = np.stack(segments, axis=0)  # (N, 24, F)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(\"Num segments:\", X.shape[0], \"Seq len:\", X.shape[1], \"Num features:\", X.shape[2])\n",
    "print(\"Label distribution:\")\n",
    "print(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06c4a0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Bearish Normal' 'Bearish Pennant' 'Bearish Wedge' 'Bullish Normal'\n",
      " 'Bullish Pennant' 'Bullish Wedge'] -> num_classes = 6\n"
     ]
    }
   ],
   "source": [
    "# Encode labels as integers\n",
    "label_values = np.sort(pd.unique(y))\n",
    "label_to_idx = {lbl: i for i, lbl in enumerate(label_values)}\n",
    "idx_to_label = {i: lbl for lbl, i in label_to_idx.items()}\n",
    "\n",
    "y_idx = np.vectorize(label_to_idx.get)(y)\n",
    "num_classes = len(label_values)\n",
    "print(\"Classes:\", label_values, \"-> num_classes =\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab245eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train segments: 46\n",
      "Val segments: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/validation split at segment level\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_idx, test_size=0.2, random_state=42, stratify=y_idx,\n",
    " )\n",
    "\n",
    "print(\"Train segments:\", X_train.shape[0])\n",
    "print(\"Val segments:\", X_val.shape[0])\n",
    "\n",
    "class SegmentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)  # (N, T, F)\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SegmentDataset(X_train, y_train)\n",
    "val_ds = SegmentDataset(X_val, y_val)\n",
    "\n",
    "batch_size = 12\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7011bc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv1d(8, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Simple CNN-based classifier for pattern recognition\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_classes: int, hidden_channels: int = 32):\n",
    "        super().__init__()\n",
    "        # Input: (B, T, F) -> rearrange to (B, F, T) for Conv1d\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_dim, out_channels=hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=hidden_channels, out_channels=hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # global average over time\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, num_classes),\n",
    "        )\n",
    "        # Apply Kaiming (He) initialization for ReLU layers\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):  # x: (B, T, F)\n",
    "        x = x.transpose(1, 2)  # (B, F, T)\n",
    "        h = self.conv(x)\n",
    "        h = self.pool(h).squeeze(-1)  # (B, C)\n",
    "        logits = self.fc(h)\n",
    "        return logits\n",
    "\n",
    "model = SimpleCNN(input_dim=X.shape[2], num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "407e39f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=1.8676 acc=0.1304 | val_loss=1.8365 acc=0.0833\n",
      "Epoch 02 | train_loss=1.7734 acc=0.2174 | val_loss=1.7998 acc=0.1667\n",
      "Epoch 03 | train_loss=1.7454 acc=0.3043 | val_loss=1.7766 acc=0.2500\n",
      "Epoch 04 | train_loss=1.7159 acc=0.2609 | val_loss=1.7622 acc=0.3333\n",
      "Epoch 05 | train_loss=1.7034 acc=0.2826 | val_loss=1.7539 acc=0.3333\n",
      "Epoch 05 | train_loss=1.7034 acc=0.2826 | val_loss=1.7539 acc=0.3333\n",
      "Epoch 06 | train_loss=1.6910 acc=0.2826 | val_loss=1.7404 acc=0.3333\n",
      "Epoch 06 | train_loss=1.6910 acc=0.2826 | val_loss=1.7404 acc=0.3333\n",
      "Epoch 07 | train_loss=1.6756 acc=0.3043 | val_loss=1.7258 acc=0.3333\n",
      "Epoch 08 | train_loss=1.6585 acc=0.3261 | val_loss=1.7142 acc=0.3333\n",
      "Epoch 07 | train_loss=1.6756 acc=0.3043 | val_loss=1.7258 acc=0.3333\n",
      "Epoch 08 | train_loss=1.6585 acc=0.3261 | val_loss=1.7142 acc=0.3333\n",
      "Epoch 09 | train_loss=1.6412 acc=0.3478 | val_loss=1.7009 acc=0.3333\n",
      "Epoch 10 | train_loss=1.6242 acc=0.3478 | val_loss=1.6907 acc=0.3333\n",
      "Epoch 09 | train_loss=1.6412 acc=0.3478 | val_loss=1.7009 acc=0.3333\n",
      "Epoch 10 | train_loss=1.6242 acc=0.3478 | val_loss=1.6907 acc=0.3333\n"
     ]
    }
   ],
   "source": [
    "# Training loop using the simple CNN\n",
    "def run_epoch(loader, model, criterion, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(yb.detach().cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    return avg_loss, acc\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = run_epoch(train_loader, model, criterion, optimizer)\n",
    "    val_loss, val_acc = run_epoch(val_loader, model, criterion, optimizer=None)\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} acc={train_acc:.4f} | val_loss={val_loss:.4f} acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685119d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final val_loss=1.6907, val_acc=0.3333\n",
      "Classification report (validation):\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Bearish Normal       0.00      0.00      0.00         1\n",
      "Bearish Pennant       0.33      1.00      0.50         3\n",
      "  Bearish Wedge       0.00      0.00      0.00         1\n",
      " Bullish Normal       0.00      0.00      0.00         3\n",
      "Bullish Pennant       1.00      0.50      0.67         2\n",
      "  Bullish Wedge       0.00      0.00      0.00         2\n",
      "\n",
      "       accuracy                           0.33        12\n",
      "      macro avg       0.22      0.25      0.19        12\n",
      "   weighted avg       0.25      0.33      0.24        12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/workenv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/conda/envs/workenv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/conda/envs/workenv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation and per-class metrics on validation set\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def get_predictions(loader):\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(xb)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(yb.numpy())\n",
    "    return np.concatenate(all_preds), np.concatenate(all_targets)\n",
    "\n",
    "# Get predictions for train and validation sets\n",
    "train_preds, train_targets = get_predictions(train_loader)\n",
    "val_preds, val_targets = get_predictions(val_loader)\n",
    "\n",
    "print(f\"Train accuracy: {accuracy_score(train_targets, train_preds):.4f}\")\n",
    "print(f\"Val accuracy: {accuracy_score(val_targets, val_preds):.4f}\")\n",
    "\n",
    "# Plot confusion matrices side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training confusion matrix\n",
    "cm_train = confusion_matrix(train_targets, train_preds)\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=label_values)\n",
    "disp_train.plot(ax=axes[0], cmap='Blues', xticks_rotation=45)\n",
    "axes[0].set_title(\"Training Confusion Matrix\")\n",
    "\n",
    "# Validation confusion matrix\n",
    "cm_val = confusion_matrix(val_targets, val_preds)\n",
    "disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=label_values)\n",
    "disp_val.plot(ax=axes[1], cmap='Blues', xticks_rotation=45)\n",
    "axes[1].set_title(\"Validation Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report for validation\n",
    "print(\"\\nClassification report (validation):\")\n",
    "print(classification_report(val_targets, val_preds, target_names=[str(lbl) for lbl in label_values]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
