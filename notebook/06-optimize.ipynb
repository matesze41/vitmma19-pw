{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dad39b4",
   "metadata": {},
   "source": [
    "# Train a small transformer model on preprocessed segments\n",
    "\n",
    "This notebook loads `segments_preproc_24.csv` produced by the preprocessing notebook, builds segment-level sequences, and trains a small transformer-based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c6660e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using preprocessed file: /work/data/export/segments_preproc_24.csv\n",
      "PyTorch Lightning version: 2.6.0\n",
      "CUDA available: True\n",
      "Wandb version: 0.23.1\n"
     ]
    }
   ],
   "source": [
    "# Imports and configuration\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, \n",
    "    ConfusionMatrixDisplay, f1_score, roc_auc_score, roc_curve, \n",
    "    auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import wandb\n",
    "\n",
    "BASE_DATA_DIR = os.path.abspath(\"../data\")\n",
    "EXPORT_DIR = os.path.join(BASE_DATA_DIR, \"export\")\n",
    "PREPROC_CSV = os.path.join(EXPORT_DIR, \"segments_preproc_24.csv\")\n",
    "\n",
    "print(\"Using preprocessed file:\", PREPROC_CSV)\n",
    "assert os.path.exists(PREPROC_CSV), f\"Preprocessed CSV not found: {PREPROC_CSV}\"\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Wandb version: {wandb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2934433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed locked to 1\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 1\n",
    "seed_everything(SEED, workers=True)\n",
    "print(f\"Random seed locked to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d497e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meznagyonkellettmz\u001b[0m (\u001b[33meznagyonkellettmz-budapesti-m-szaki-s-gazdas-gtudom-nyi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meznagyonkellettmz\u001b[0m (\u001b[33meznagyonkellettmz-budapesti-m-szaki-s-gazdas-gtudom-nyi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Weights & Biases!\n"
     ]
    }
   ],
   "source": [
    "# Wandb authentication - paste your token here\n",
    "WANDB_API_TOKEN = \"e5e50a1ad57d78a1def2302321a9d83243fe6fd8\"  # Replace with your actual token\n",
    "\n",
    "# Login to wandb\n",
    "wandb.login(key=WANDB_API_TOKEN)\n",
    "print(\"Successfully logged in to Weights & Biases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29529c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary metric: PR-AUC (macro)\n",
      "Description: Precision-Recall curve area (macro): better for imbalanced classes\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation configuration\n",
    "# Change this single variable to switch the primary metric for model selection\n",
    "# Options: 'auc_ovo', 'auc_ovr', 'f1', 'accuracy', 'pr_auc'\n",
    "PRIMARY_METRIC = 'pr_auc'\n",
    "\n",
    "# Metric display names and whether higher is better\n",
    "METRIC_CONFIG = {\n",
    "    'auc_ovo': {\n",
    "        'name': 'AUC-ROC (OvO)',\n",
    "        'short': 'ovo',\n",
    "        'higher_is_better': True,\n",
    "        'monitor': 'val_auc_ovo',\n",
    "        'description': 'One-vs-One: evaluates all pairwise class comparisons'\n",
    "    },\n",
    "    'auc_ovr': {\n",
    "        'name': 'AUC-ROC (OvR)',\n",
    "        'short': 'ovr',\n",
    "        'higher_is_better': True,\n",
    "        'monitor': 'val_auc_ovr',\n",
    "        'description': 'One-vs-Rest: evaluates each class vs all others'\n",
    "    },\n",
    "    'f1': {\n",
    "        'name': 'F1 Score (macro)',\n",
    "        'short': 'f1',\n",
    "        'higher_is_better': True,\n",
    "        'monitor': 'val_f1',\n",
    "        'description': 'Harmonic mean of precision and recall'\n",
    "    },\n",
    "    'accuracy': {\n",
    "        'name': 'Accuracy',\n",
    "        'short': 'acc',\n",
    "        'higher_is_better': True,\n",
    "        'monitor': 'val_accuracy',\n",
    "        'description': 'Proportion of correct predictions'\n",
    "    },\n",
    "    'pr_auc': {\n",
    "        'name': 'PR-AUC (macro)',\n",
    "        'short': 'pr',\n",
    "        'higher_is_better': True,\n",
    "        'monitor': 'val_pr_auc',\n",
    "        'description': 'Precision-Recall curve area (macro): better for imbalanced classes'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Primary metric: {METRIC_CONFIG[PRIMARY_METRIC]['name']}\")\n",
    "print(f\"Description: {METRIC_CONFIG[PRIMARY_METRIC]['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7431e368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw preprocessed shape: (3456, 12)\n",
      "Feature columns ( 8 ): ['open_norm', 'high_norm', 'low_norm', 'close_norm', 'vol_close', 'vol_high_low', 'compression_ratio', 'trend']\n",
      "Num segments: 144 Seq len: 24 Num features: 8\n",
      "Label distribution:\n",
      "Bullish Normal     40\n",
      "Bearish Normal     27\n",
      "Bearish Pennant    26\n",
      "Bullish Pennant    22\n",
      "Bullish Wedge      15\n",
      "Bearish Wedge      14\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed dataset and build segment-level sequences\n",
    "df = pd.read_csv(PREPROC_CSV)\n",
    "print(\"Raw preprocessed shape:\", df.shape)\n",
    "\n",
    "# Ensure correct ordering within each segment\n",
    "df = df.sort_values([\"segment_id\", \"seq_pos\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in [\"segment_id\", \"label\", \"csv_file\", \"seq_pos\"]]\n",
    "print(\"Feature columns (\", len(feature_cols), \"):\", feature_cols)\n",
    "\n",
    "# Group into (segment, sequence of length 24, label)\n",
    "segments = []\n",
    "labels = []\n",
    "\n",
    "for seg_id, g in df.groupby(\"segment_id\", sort=True):\n",
    "    g = g.sort_values(\"seq_pos\", kind=\"mergesort\")\n",
    "    feat = g[feature_cols].to_numpy(dtype=np.float32)\n",
    "    # Expect 24 steps; if shorter/longer, adjust with simple strategies\n",
    "    if feat.shape[0] < 24:\n",
    "        # pad by repeating last step\n",
    "        pad = np.repeat(feat[-1:, :], 24 - feat.shape[0], axis=0)\n",
    "        feat = np.concatenate([feat, pad], axis=0)\n",
    "    elif feat.shape[0] > 24:\n",
    "        # truncate extra steps\n",
    "        feat = feat[:24, :]\n",
    "\n",
    "    assert feat.shape[0] == 24, feat.shape\n",
    "    segments.append(feat)\n",
    "    labels.append(g[\"label\"].iloc[0])\n",
    "\n",
    "X = np.stack(segments, axis=0)  # (N, 24, F)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(\"Num segments:\", X.shape[0], \"Seq len:\", X.shape[1], \"Num features:\", X.shape[2])\n",
    "print(\"Label distribution:\")\n",
    "print(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06c4a0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Bearish Normal' 'Bearish Pennant' 'Bearish Wedge' 'Bullish Normal'\n",
      " 'Bullish Pennant' 'Bullish Wedge'] -> num_classes = 6\n"
     ]
    }
   ],
   "source": [
    "# Encode labels as integers\n",
    "label_values = np.sort(pd.unique(y))\n",
    "label_to_idx = {lbl: i for i, lbl in enumerate(label_values)}\n",
    "idx_to_label = {i: lbl for lbl, i in label_to_idx.items()}\n",
    "\n",
    "y_idx = np.vectorize(label_to_idx.get)(y)\n",
    "num_classes = len(label_values)\n",
    "print(\"Classes:\", label_values, \"-> num_classes =\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab245eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train segments: 115\n",
      "Val segments: 29\n",
      "Class distribution: [27 26 14 40 22 15]\n",
      "Class weights: [0.7823394  0.81242937 1.50879741 0.52807909 0.96014381 1.40821092]\n"
     ]
    }
   ],
   "source": [
    "# Train/validation split at segment level\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_idx, test_size=0.2, random_state=SEED, stratify=y_idx,\n",
    " )\n",
    "\n",
    "print(\"Train segments:\", X_train.shape[0])\n",
    "print(\"Val segments:\", X_val.shape[0])\n",
    "\n",
    "class SegmentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)  # (N, T, F)\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SegmentDataset(X_train, y_train)\n",
    "val_ds = SegmentDataset(X_val, y_val)\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "class_counts = np.bincount(y_idx)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "print(f\"Class distribution: {class_counts}\")\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7011bc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlagPatternClassifier(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv1d(8, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.1, inplace=False)\n",
      "    (12): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "Total parameters: 99,238\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Lightning Module with Wandb integration\n",
    "class FlagPatternClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_classes: int,\n",
    "        class_weights: np.ndarray,\n",
    "        hidden_channels: int = 64,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 1e-4,\n",
    "        batch_size: int = 12\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['class_weights'])\n",
    "        \n",
    "        # Model architecture\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(hidden_channels * 2, hidden_channels * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_channels * 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels // 2, num_classes),\n",
    "        )\n",
    "        \n",
    "        # Loss function with class weights\n",
    "        self.class_weights = torch.FloatTensor(class_weights)\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        \n",
    "        # Store predictions for epoch-end metrics\n",
    "        self.validation_step_outputs = []\n",
    "        self.training_step_outputs = []\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # (B, T, F) -> (B, F, T)\n",
    "        h = self.conv(x)\n",
    "        h = self.pool(h).squeeze(-1)\n",
    "        logits = self.fc(h)\n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        self.training_step_outputs.append({\n",
    "            'loss': loss,\n",
    "            'preds': preds.detach().cpu(),\n",
    "            'probs': probs.detach().cpu(),\n",
    "            'targets': y.detach().cpu()\n",
    "        })\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        self.validation_step_outputs.append({\n",
    "            'loss': loss,\n",
    "            'preds': preds.detach().cpu(),\n",
    "            'probs': probs.detach().cpu(),\n",
    "            'targets': y.detach().cpu()\n",
    "        })\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self._compute_epoch_metrics(self.training_step_outputs, 'train')\n",
    "        self.training_step_outputs.clear()\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self._compute_epoch_metrics(self.validation_step_outputs, 'val')\n",
    "        self.validation_step_outputs.clear()\n",
    "    \n",
    "    def _compute_epoch_metrics(self, outputs, prefix):\n",
    "        all_preds = torch.cat([x['preds'] for x in outputs]).numpy()\n",
    "        all_probs = torch.cat([x['probs'] for x in outputs]).numpy()\n",
    "        all_targets = torch.cat([x['targets'] for x in outputs]).numpy()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "        \n",
    "        try:\n",
    "            auc_ovo = roc_auc_score(all_targets, all_probs, multi_class='ovo', average='macro')\n",
    "            auc_ovr = roc_auc_score(all_targets, all_probs, multi_class='ovr', average='macro')\n",
    "        except ValueError:\n",
    "            auc_ovo = 0.0\n",
    "            auc_ovr = 0.0\n",
    "        \n",
    "        try:\n",
    "            y_bin = label_binarize(all_targets, classes=range(all_probs.shape[1]))\n",
    "            pr_auc_per_class = []\n",
    "            for i in range(all_probs.shape[1]):\n",
    "                pr_auc_per_class.append(average_precision_score(y_bin[:, i], all_probs[:, i]))\n",
    "            pr_auc = np.mean(pr_auc_per_class)\n",
    "        except ValueError:\n",
    "            pr_auc = 0.0\n",
    "        \n",
    "        # Log all metrics (wandb will automatically capture these)\n",
    "        self.log(f'{prefix}_accuracy', accuracy, prog_bar=True)\n",
    "        self.log(f'{prefix}_f1', f1, prog_bar=True)\n",
    "        self.log(f'{prefix}_auc_ovo', auc_ovo, prog_bar=True)\n",
    "        self.log(f'{prefix}_auc_ovr', auc_ovr, prog_bar=True)\n",
    "        self.log(f'{prefix}_pr_auc', pr_auc, prog_bar=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=20\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch'\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initial model creation for reference (will be recreated during sweep)\n",
    "batch_size = 12\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "model = FlagPatternClassifier(\n",
    "    input_dim=X.shape[2],\n",
    "    num_classes=num_classes,\n",
    "    class_weights=class_weights,\n",
    "    hidden_channels=64,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f65c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep configuration:\n",
      "  Optimization metric: val_pr_auc (maximize)\n",
      "  Method: bayes\n",
      "  Hyperparameters to optimize:\n",
      "    - Learning rate: 0.0001 to 0.01\n",
      "    - Batch size: [8, 12, 16, 24, 32]\n",
      "    - Hidden channels: [32, 64, 128, 256]\n",
      "    - Weight decay: 1e-05 to 0.001\n"
     ]
    }
   ],
   "source": [
    "# Wandb Sweep Configuration\n",
    "# The sweep will optimize for val_pr_auc (validation PR-AUC)\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Bayesian optimization\n",
    "    'metric': {\n",
    "        'name': 'val_pr_auc',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-4,\n",
    "            'max': 1e-2\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [8, 12, 16, 24, 32]\n",
    "        },\n",
    "        'hidden_channels': {\n",
    "            'values': [32, 64, 128,]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-3\n",
    "        },\n",
    "        'max_epochs': {\n",
    "            'value': 60  # Fixed value\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Sweep configuration:\")\n",
    "print(f\"  Optimization metric: {sweep_config['metric']['name']} ({sweep_config['metric']['goal']})\")\n",
    "print(f\"  Method: {sweep_config['method']}\")\n",
    "print(f\"  Hyperparameters to optimize:\")\n",
    "print(f\"    - Learning rate: {sweep_config['parameters']['lr']['min']} to {sweep_config['parameters']['lr']['max']}\")\n",
    "print(f\"    - Batch size: {sweep_config['parameters']['batch_size']['values']}\")\n",
    "print(f\"    - Hidden channels: {sweep_config['parameters']['hidden_channels']['values']}\")\n",
    "print(f\"    - Weight decay: {sweep_config['parameters']['weight_decay']['min']} to {sweep_config['parameters']['weight_decay']['max']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "407e39f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined. Ready to start sweep!\n"
     ]
    }
   ],
   "source": [
    "# Training function for wandb sweep\n",
    "def train_sweep():\n",
    "    \"\"\"\n",
    "    Training function called by wandb agent for each sweep run.\n",
    "    Wandb automatically injects hyperparameters via wandb.config.\n",
    "    \"\"\"\n",
    "    # Initialize wandb run\n",
    "    with wandb.init() as run:\n",
    "        # Get hyperparameters from wandb config\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Create data loaders with sweep batch size\n",
    "        sweep_train_loader = DataLoader(\n",
    "            train_ds, \n",
    "            batch_size=config.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=0\n",
    "        )\n",
    "        sweep_val_loader = DataLoader(\n",
    "            val_ds, \n",
    "            batch_size=config.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        # Create model with sweep hyperparameters\n",
    "        sweep_model = FlagPatternClassifier(\n",
    "            input_dim=X.shape[2],\n",
    "            num_classes=num_classes,\n",
    "            class_weights=class_weights,\n",
    "            hidden_channels=config.hidden_channels,\n",
    "            lr=config.lr,\n",
    "            weight_decay=config.weight_decay,\n",
    "            batch_size=config.batch_size\n",
    "        )\n",
    "        \n",
    "        # Setup wandb logger\n",
    "        wandb_logger = WandbLogger(\n",
    "            project='flag-pattern-classifier',\n",
    "            log_model=False  # Don't save models during sweep to save space\n",
    "        )\n",
    "        \n",
    "        # Setup checkpoint callback (based on val_pr_auc)\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=os.path.join(EXPORT_DIR, \"checkpoints\", f\"sweep_{run.id}\"),\n",
    "            filename='best_model_{epoch:02d}_{val_pr_auc:.4f}',\n",
    "            monitor='val_pr_auc',\n",
    "            mode='max',\n",
    "            save_top_k=1,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Early stopping to prevent wasting time on bad runs\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor='val_pr_auc',\n",
    "            patience=10,\n",
    "            mode='max',\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        sweep_trainer = pl.Trainer(\n",
    "            max_epochs=config.max_epochs,\n",
    "            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "            devices=1,\n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "            logger=wandb_logger,\n",
    "            deterministic=True,\n",
    "            log_every_n_steps=10,\n",
    "            enable_progress_bar=False,  # Disable for cleaner sweep output\n",
    "            enable_model_summary=False\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        sweep_trainer.fit(sweep_model, sweep_train_loader, sweep_val_loader)\n",
    "        \n",
    "        # Log best score\n",
    "        print(f\"Run {run.id}: Best val_pr_auc = {checkpoint_callback.best_model_score:.4f}\")\n",
    "\n",
    "print(\"Training function defined. Ready to start sweep!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3052f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run the wandb sweep\n",
    "# This will create a sweep on wandb servers and run the agent locally\n",
    "\n",
    "# Create the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project='flag-pattern-classifier')\n",
    "print(f\"Sweep created with ID: {sweep_id}\")\n",
    "print(f\"View sweep at: https://wandb.ai/[your-username]/flag-pattern-classifier/sweeps/{sweep_id}\")\n",
    "\n",
    "# Run the sweep agent\n",
    "# count=10 means it will run 10 different hyperparameter configurations\n",
    "# You can adjust this number or remove it to run indefinitely until stopped\n",
    "wandb.agent(sweep_id, function=train_sweep, count=100)\n",
    "\n",
    "print(\"\\nSweep completed! Check your wandb dashboard for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1471fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best run from the sweep (run this after the sweep completes)\n",
    "api = wandb.Api()\n",
    "\n",
    "# You'll need to get your username from wandb - you can find it in your dashboard URL\n",
    "# or run: wandb.Api().viewer()['entity']\n",
    "wandb_username = api.viewer()['entity']\n",
    "sweep_path = f\"{wandb_username}/flag-pattern-classifier/{sweep_id}\"\n",
    "\n",
    "print(f\"Fetching sweep: {sweep_path}\")\n",
    "sweep = api.sweep(sweep_path)\n",
    "best_run = sweep.best_run()\n",
    "\n",
    "print(\"\\nBest run configuration:\")\n",
    "print(f\"  Run ID: {best_run.id}\")\n",
    "print(f\"  Run name: {best_run.name}\")\n",
    "print(f\"  Best val_pr_auc: {best_run.summary.get('val_pr_auc', 'N/A')}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for param in ['lr', 'batch_size', 'hidden_channels', 'weight_decay']:\n",
    "    print(f\"  {param}: {best_run.config.get(param, 'N/A')}\")\n",
    "\n",
    "print(f\"\\nAll metrics:\")\n",
    "print(f\"  Accuracy: {best_run.summary.get('val_accuracy', 'N/A'):.4f}\")\n",
    "print(f\"  F1: {best_run.summary.get('val_f1', 'N/A'):.4f}\")\n",
    "print(f\"  PR-AUC: {best_run.summary.get('val_pr_auc', 'N/A'):.4f}\")\n",
    "print(f\"  AUC-OvO: {best_run.summary.get('val_auc_ovo', 'N/A'):.4f}\")\n",
    "print(f\"  AUC-OvR: {best_run.summary.get('val_auc_ovr', 'N/A'):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
